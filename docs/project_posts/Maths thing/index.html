<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Siddharth D">
<meta name="dcterms.date" content="2024-01-05">

<title>Siddharth D - QuadCopter Reinforcement learning Bot</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"express",
  "palette":"dark",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta name="twitter:title" content="Siddharth D - QuadCopter Reinforcement learning Bot">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="image.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Siddharth D</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blogs.html" rel="" target="">
 <span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../contact.html" rel="" target="">
 <span class="menu-text">Contact</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">QuadCopter Reinforcement learning Bot</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">project</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Siddharth D </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 5, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#environment" id="toc-environment" class="nav-link" data-scroll-target="#environment">Environment</a>
  <ul class="collapse">
  <li><a href="#physics" id="toc-physics" class="nav-link" data-scroll-target="#physics">Physics</a></li>
  <li><a href="#scoring" id="toc-scoring" class="nav-link" data-scroll-target="#scoring">Scoring</a></li>
  </ul></li>
  <li><a href="#agents" id="toc-agents" class="nav-link" data-scroll-target="#agents">Agents</a>
  <ul class="collapse">
  <li><a href="#human-agent" id="toc-human-agent" class="nav-link" data-scroll-target="#human-agent">Human Agent</a></li>
  <li><a href="#pid-agent" id="toc-pid-agent" class="nav-link" data-scroll-target="#pid-agent">PID Agent</a></li>
  <li><a href="#reinforcement-learning-agent" id="toc-reinforcement-learning-agent" class="nav-link" data-scroll-target="#reinforcement-learning-agent">Reinforcement learning Agent</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1">Introduction</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#pid" id="toc-pid" class="nav-link" data-scroll-target="#pid">PID</a>
  <ul class="collapse">
  <li><a href="#p" id="toc-p" class="nav-link" data-scroll-target="#p">P</a></li>
  <li><a href="#p-i" id="toc-p-i" class="nav-link" data-scroll-target="#p-i">P-I</a></li>
  <li><a href="#p-d" id="toc-p-d" class="nav-link" data-scroll-target="#p-d">P-D</a></li>
  <li><a href="#p-i-d" id="toc-p-i-d" class="nav-link" data-scroll-target="#p-i-d">P-I-D</a></li>
  </ul></li>
  <li><a href="#mdp" id="toc-mdp" class="nav-link" data-scroll-target="#mdp">MDP</a>
  <ul class="collapse">
  <li><a href="#formulation" id="toc-formulation" class="nav-link" data-scroll-target="#formulation">Formulation</a>
  <ul class="collapse">
  <li><a href="#the-bellman-equation" id="toc-the-bellman-equation" class="nav-link" data-scroll-target="#the-bellman-equation">The Bellman Equation</a></li>
  </ul></li>
  <li><a href="#value-iteration" id="toc-value-iteration" class="nav-link" data-scroll-target="#value-iteration">Value Iteration</a></li>
  <li><a href="#theorem" id="toc-theorem" class="nav-link" data-scroll-target="#theorem">Theorem</a>
  <ul class="collapse">
  <li><a href="#theorem-statement" id="toc-theorem-statement" class="nav-link" data-scroll-target="#theorem-statement">Theorem Statement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#q-learning" id="toc-q-learning" class="nav-link" data-scroll-target="#q-learning">Q-learning</a>
  <ul class="collapse">
  <li><a href="#introduction-2" id="toc-introduction-2" class="nav-link" data-scroll-target="#introduction-2">Introduction</a></li>
  <li><a href="#formulation-1" id="toc-formulation-1" class="nav-link" data-scroll-target="#formulation-1">Formulation</a></li>
  </ul></li>
  <li><a href="#sac" id="toc-sac" class="nav-link" data-scroll-target="#sac">SAC</a>
  <ul class="collapse">
  <li><a href="#simple-overview" id="toc-simple-overview" class="nav-link" data-scroll-target="#simple-overview">Simple overview</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The goal of this project is to compare three approaches to control a Quadcopter in the task of navigating to multiple waypoints without crashing</p>
<ul>
<li>Human Agents: A human directly controlls the drone and drives it to the destination using keyboard input</li>
<li>PID: The Quadcopter is navigated autonomously using the feedback loop mechanism of the PID controller</li>
<li>Reinforcement Learning Agent: This being the crux of the project using 2 reinforcement learning algorithms DQN (Q learning and SAC). Detailed explanation about Q learning is what is being concentrated more on the project.</li>
</ul>
<p>Quadcoptes are generally controlled either by humans or by use of algorithms derived by control theory. PID controllers are often used to control the motor power of each propeller and by sensing information the balace is maintainted using feedback loop mechanism</p>
<p>Using reinforcement learning learning will be the next step in the evolution of autonomous vehicles. Given the state of the vehicle and the environment, the agent will learn to take actions that will maximize the reward. The agent will learn to navigate to the destination without crashing.</p>
</section>
<section id="environment" class="level1">
<h1>Environment</h1>
<p>Our environment uses OPENAI gym to simulate the quadcopter. The environment is a 2D space with 2 propellers. The environment is initialized with a random position and the agent should reach the destination (here balloon) without crashing. The agent is given all the details such as:</p>
<ul>
<li>angle_to_up : angle between the drone and the up vector (to observe gravity)</li>
<li>velocity : velocity of the drone</li>
<li>angle_velocity : angle of the velocity vector</li>
<li>distance_to_target : distance to the target</li>
<li>angle_to_target : angle between the drone and the target</li>
<li>angle_target_and_velocity : angle between the to_target vector and the velocity vector</li>
<li>distance_to_target : distance to the target</li>
</ul>
<p>Hence completing the physics of the environment.</p>
<p><strong>Say about openai GYM</strong> OPEN AI gym is a toolkit for developing and comparing reinforcement learning algorithms. It is a python library that provides a variety of environments to test our reinforcement learning algorithms. It is a collection of test problems that are called environments. Each environment has its own purpose. The environment used in this project is the classic cartpole environment. The environment is a 2D space with 2 propellers. We use openai gym here because the environments provides features such as rendering the environment, getting the state of the environment, taking actions, etc.</p>
<section id="physics" class="level2">
<h2 class="anchored" data-anchor-id="physics">Physics</h2>
<p>At each step of the simulation, the agents have to provide values between -0.003 and 0.083 (values chosen arbitrarily) for the thrust of their left and right rotors. The thrust is then converted to a force vector that is applied to the drone. The drone is then moved according to the force vector and the physics of the environment. The physics of the environment is as follows:</p>
<p>Step 1: Calculate acceleration</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \ddot{x}=\frac{-\left(T_L+T_\gamma\right) \sin (\theta)}{m} \\
&amp; \ddot{y}=\frac{-\left(T_L+T_\gamma\right) \cos (\theta)}{m} \\
&amp; \ddot{\theta}=\frac{\left(T_r-T_L\right) \cdot L}{m}
\end{aligned}
\]</span></p>
<p>Equations for acceleration <span class="math inline">\(( \ddot{x}, \ddot{y}, \ddot{\theta} )\)</span>: <span class="math inline">\(\ddot{x} = \frac{T_L + T_R}{m} \cos(\theta)\)</span> <span class="math inline">\(\ddot{y} = \frac{T_L + T_R}{m} \sin(\theta)\)</span> <span class="math inline">\(\ddot{\theta} = \frac{l}{I} (T_R - T_L)\)</span></p>
<p>Definitions of variables:</p>
<ul>
<li><span class="math inline">\(\ddot{x}, \ddot{y}\)</span>: acceleration on the x and y axes.</li>
<li><span class="math inline">\((\ddot{\theta})\)</span>: angular acceleration.</li>
<li><span class="math inline">\((T_L, T_R)\)</span>: input thrust for the left and right propellers.</li>
<li><span class="math inline">\((\theta)\)</span>: angle of the drone with respect to the z-axis.</li>
<li><span class="math inline">\((m)\)</span>: mass of the drone.</li>
<li><span class="math inline">\((g)\)</span>: acceleration due to gravity.</li>
<li><span class="math inline">\((l):\)</span> distance between the center of mass and the propeller</li>
</ul>
<p>Step 2: Derive the speed and the position.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \dot{x}(t-1)=\ddot{x} \cdot d t+\dot{x}(t) \\
&amp; x(t+1)=\dot{x} \cdot d t+x(t
\end{aligned}
\]</span></p>
</section>
<section id="scoring" class="level2">
<h2 class="anchored" data-anchor-id="scoring">Scoring</h2>
<p>The simulation runs at 60fps for 100 seconds (can be modified). The agent should reach the destination without crashing. If the agents goes out of the field of the simulation, it is considered as a crash. The agent respawns with a penalty wait time of 3 seconds per crash (this is a bad thing in competitive environments).</p>
</section>
</section>
<section id="agents" class="level1">
<h1>Agents</h1>
<section id="human-agent" class="level2">
<h2 class="anchored" data-anchor-id="human-agent">Human Agent</h2>
<p>The human agent is simulated in a near Earth environment with the thrust being constant and the human controls by using his keyboard.</p>
<p>The values are:</p>
<ol type="1">
<li>Thrust initialization: <span class="math inline">\(T_L = 0.04 \quad \text{and} \quad T_R = 0.04\)</span></li>
<li>UP (incrementing <span class="math inline">\(T_L\)</span> and <span class="math inline">\(T_R\)</span> by 0.04): <span class="math inline">\(T_L = T_L + 0.04 \quad \text{and} \quad T_R = T_R + 0.04\)</span></li>
<li>DOWN (decrementing <span class="math inline">\(T_L\)</span> and <span class="math inline">\(T_R\)</span> by 0.04): <span class="math inline">\(T_L = T_L - 0.04 \quad \text{and} \quad T_R = T_R - 0.04\)</span></li>
<li>LEFT: <span class="math inline">\(T_L = T_L - 0.003 \quad \text{and} \quad T_R = T_R + 0.04\)</span></li>
<li>RIGHT: <span class="math inline">\(T_L = T_L + 0.004 \quad \text{and} \quad T_R = T_R - 0.03\)</span></li>
</ol>
</section>
<section id="pid-agent" class="level2">
<h2 class="anchored" data-anchor-id="pid-agent">PID Agent</h2>
<p>For the PID agent we assume the error for the position from the target it has to reach and the location of the drone and move it vertically or rotate it at certain angle to move at different position</p>
<ul>
<li>The vertical distance from the drone to target is sent to PID which outputs a optimal vertical speed</li>
<li>The horizontal distance is used to modify the thrust of each rotor to move it in the left or right direction same as the HUMAN Agent the rotation logic remains same</li>
</ul>
<p>—- PUT IMAGE HERE ——-</p>
</section>
<section id="reinforcement-learning-agent" class="level2">
<h2 class="anchored" data-anchor-id="reinforcement-learning-agent">Reinforcement learning Agent</h2>
<p>The environment in which the drone trains is called as DRONEENV and we use openAI gym for creating the agent. The observations are made easy for the agent to understand. For observation the agents is given the following information (current position and the speed to move is determined by the following inputs)</p>
<ul>
<li>angle_to_up : angle between the drone and the up vector (to observe gravity)</li>
<li>velocity : velocity of the drone</li>
<li>angle_velocity : angle of the velocity vector</li>
<li>distance_to_target : distance to the target</li>
<li>angle_to_target : angle between the drone and the target</li>
<li>angle_target_and_velocity : angle between the to_target vector and the velocity vector</li>
<li>distance_to_target : distance to the target</li>
</ul>
<p>Before going deeper into this we must know some basic concepts of Reinforcement Learning</p>
<section id="introduction-1" class="level3">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>
<p>Reinforcement learning is an area of Machine Learning. It is about taking suitable action to maximize reward in a particular situation. It is employed by various software and machines to find the best possible behavior or path it should take in a specific situation. Reinforcement learning differs from supervised learning in a way that in supervised learning the training data has the answer key with it so the model is trained with the correct answer itself whereas in reinforcement learning, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of a training dataset, it is bound to learn from its experience.</p>
<p>Reinforcement Learning (RL) is the science of decision making. It is about learning the optimal behavior in an environment to obtain maximum reward. In RL, the data is accumulated from machine learning systems that use a trial-and-error method. Data is not part of the input that we would find in supervised or unsupervised machine learning.</p>
<p>We have 4 main components when it comes to reinforcement learning.</p>
<section id="reward" class="level4">
<h4 class="anchored" data-anchor-id="reward">Reward</h4>
<ul>
<li>The rewards are scalar quantity which is a single number which can range from [−N,N] and sometimes it can have multiple outcomes so choosing them is the job of the agent</li>
<li>The reward should be outside the control of the agent which means that the reward should come from the environment in which the agent is training this will make sure that the agent wont be able to simply add more reward</li>
<li>The reward depends on the domain</li>
<li>Having singular scalar value is good as its easy to estimate the model performance</li>
<li>Reward should be bounded that is from -N to N</li>
<li>Rewards must be given for each epoch or every single iteration</li>
</ul>
</section>
<section id="policy" class="level4">
<h4 class="anchored" data-anchor-id="policy">Policy</h4>
<p>A policy is a rule that determines how an agent chooses an action in each state. It can be deterministic or stochastic, depending on whether the agent always selects the same action or randomly picks an action according to some probability distribution. Policy is a conditional probability which is given by <span class="math inline">\(\pi \left( a| s\right) =P( A_{t}= a| s_{t}= s)\)</span> -&gt; The following equation stands for the probablity that the agent will take action ‘A’ given that at time t the state is ‘S’</p>
</section>
<section id="return" class="level4">
<h4 class="anchored" data-anchor-id="return">Return</h4>
<p>Return is a measure of the total reward that an agent can expect to receive in the future, starting from a given state or state-action pair. It is often used as a target for learning the value functions that estimate how good a state or an action is. There are different ways to define return, depending on how the rewards are discounted or summed up. One common way is to use the discounted return, which is given by the formula:</p>
<p><span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\]</span></p>
<ul>
<li>where <span class="math inline">\(G_t\)</span> is the return at time step <span class="math inline">\(t\)</span>, <span class="math inline">\(R_{t+k+1}\)</span> is the reward received at time step <span class="math inline">\(t+k+1\)</span>, and <span class="math inline">\(\gamma\)</span> is a discount factor between 0 and 1 that determines how much the agent values the future rewards.</li>
<li>Agent which uses only the first term is called as near-sighted agent.</li>
</ul>
</section>
<section id="value-function" class="level4">
<h4 class="anchored" data-anchor-id="value-function">Value Function</h4>
<p>The value function <span class="math inline">\(V^\pi(s)\)</span> is the expected value of the return <span class="math inline">\(G_t\)</span> when the agent follows the policy <span class="math inline">\(\pi\)</span> and starts from the state <span class="math inline">\(s\)</span>.</p>
<p><span class="math display">\[
V^\pi(s) = \mathbb{E}_\pi \left\{ G_t \mid S_t = s \right\}
\]</span></p>
<p>The expectation <span class="math inline">\(\mathbb{E}_\pi\)</span> is taken over the distribution of the possible trajectories that the agent can experience under the policy <span class="math inline">\(\pi\)</span>. The value function represents how good or desirable a state is for the agent, and it can be used to evaluate or improve the policy.</p>
</section>
</section>
</section>
</section>
<section id="pid" class="level1">
<h1>PID</h1>
<section id="p" class="level2">
<h2 class="anchored" data-anchor-id="p">P</h2>
</section>
<section id="p-i" class="level2">
<h2 class="anchored" data-anchor-id="p-i">P-I</h2>
</section>
<section id="p-d" class="level2">
<h2 class="anchored" data-anchor-id="p-d">P-D</h2>
</section>
<section id="p-i-d" class="level2">
<h2 class="anchored" data-anchor-id="p-i-d">P-I-D</h2>
</section>
</section>
<section id="mdp" class="level1">
<h1>MDP</h1>
<p>Markov Decision Process is here used to characterize the the problem the Markov chain for finding the next state transition with respect to the expected reward is given by the following equation</p>
<p><span class="math display">\[
P\left(S_{t+1}, R_{t+1} \mid S_t, A_t, S_{t-1}, A_{t-1} \ldots S_0\right)
\]</span> So for optimally finding the next state we must know everything that has happened before This formulation can be written as following: <span class="math display">\[
P\left(S_{t+1}, R_{t+1} \mid S_t, A_t\right)
\]</span></p>
<section id="formulation" class="level2">
<h2 class="anchored" data-anchor-id="formulation">Formulation</h2>
<p>In our research, we make certain assumptions to simplify the calculations and make them more static. The primary assumption is given by the following equations:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; P(S_{t+1}=S^{\prime}, R_{t+1}=v \mid S_t=S, A_t=a) -&gt; {1} \\
&amp; P(S^{\prime}, R \mid S, a) -&gt; {2}
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(P(S^{\prime}, R \mid S, a)\)</span> represents the stationary assumption.</p>
<p>To obtain the output, we need:</p>
<ul>
<li><span class="math inline">\(S\)</span>, which represents the set of states, also known as the state space.</li>
<li><span class="math inline">\(A\)</span>, which represents the set of actions.</li>
<li><span class="math inline">\(\gamma\)</span>, which is used when we are considering discounted returns.</li>
</ul>
<p>Instead of using a single joint distribution given by the equation, we can break down the equation into two parts:</p>
<ul>
<li>Transition probabilities <span class="math inline">\(P\left(s^{\prime} \mid s, A\right)\)</span></li>
<li>Expected reward <span class="math inline">\(E\left(r \mid s, a, s^{\prime}\right)\)</span></li>
</ul>
<p>We write it down to break down the joint distribution. Here, only <span class="math inline">\(E\left(r \mid s, a, s^{\prime}\right)\)</span> is enough as we are optimizing rewards.</p>
<p>The formulation of Markov Decision Process (MDP) is given by the Bellman equation:</p>
<p><span class="math display">\[
S, A, P\left(s^{\prime} \mid S, A\right), E\left(r \mid s, a, S^{\prime}\right)
\]</span></p>
<section id="the-bellman-equation" class="level3">
<h3 class="anchored" data-anchor-id="the-bellman-equation">The Bellman Equation</h3>
<p>The Bellman equation is central to the theory of MDPs. It provides a recursive formulation for the value function of a policy in an MDP. The value function represents the expected return (accumulated rewards) for an agent starting in a particular state and following a specific policy thereafter.</p>
<p>The Bellman equation for MDPs can be expressed in two forms: one for the state-value function <code>V(s)</code> and another for the action-value function <code>Q(s, a)</code>:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; V(s) = \max_a \left\{R(s,a) + \gamma \sum P(s'|s,a) V(s')\right\}  -&gt; {3} \\
&amp; Q(s,a) = R(s,a) + \gamma \sum P(s'|s,a) \max_{a'} Q(s',a')  -&gt; {4}
\end{aligned}
\]</span></p>
<p>In these equations:</p>
<ul>
<li><code>V(s)</code> is the value of state <code>s</code> under a policy.</li>
<li><code>Q(s, a)</code> is the value of taking action <code>a</code> in state <code>s</code> under a policy.</li>
<li><code>R(s, a)</code> is the immediate reward received after transitioning from state <code>s</code> with action <code>a</code>.</li>
<li><code>P(s'|s, a)</code> is the transition probability of landing in state <code>s'</code> when action <code>a</code> is taken in state <code>s</code>.</li>
<li><code>γ</code> is the discount factor which determines the present value of future rewards.</li>
</ul>
<p>The Bellman equation essentially states that the value of a state (or state-action pair) is equal to the immediate reward plus the discounted expected value of the next state (or state-action pair). This recursive nature of the Bellman equation allows us to solve MDPs using dynamic programming methods.</p>
</section>
</section>
<section id="value-iteration" class="level2">
<h2 class="anchored" data-anchor-id="value-iteration">Value Iteration</h2>
<ul>
<li>Value function become useful when the Markov property holds true because we assume value to state regardless of how we came to that particular state</li>
<li>If Markov property is not met how we arrived at the state will influence what will happen in future</li>
<li>In the Value iteration approach we wont need the past states</li>
</ul>
<p>Value iteration is a powerful approach that eliminates the need for past states in the computation of the value function. This method is widely employed in reinforcement learning and dynamic programming.</p>
</section>
<section id="theorem" class="level2">
<h2 class="anchored" data-anchor-id="theorem">Theorem</h2>
<p>Let <span class="math inline">\(v^0 \in V\)</span> and <span class="math inline">\(\varepsilon &gt; 0\)</span> be given. We derive the sequence <span class="math inline">\(\{v^n\}\)</span> from <span class="math inline">\(v^{n+1} = LV^n\)</span>, where <span class="math inline">\(LV = \max_{\pi}\{r + \gamma P^{\pi}v\}\)</span>. Here, <span class="math inline">\(r\)</span> represents the reward, <span class="math inline">\(\gamma\)</span> is the discount factor, and <span class="math inline">\(P^{\pi}\)</span> is the transition probability under policy <span class="math inline">\(\pi\)</span>.</p>
<section id="theorem-statement" class="level3">
<h3 class="anchored" data-anchor-id="theorem-statement">Theorem Statement</h3>
<p>Then:</p>
<ol type="a">
<li><p><span class="math inline">\(v^n\)</span> converges in the norm to <span class="math inline">\(v^*\)</span>.</p></li>
<li><p>For all finite <span class="math inline">\(N\)</span> at which the conditions <span class="math display">\[
\|v^{n+1} - v^n\| &lt; \varepsilon \times \frac{(1-r)}{2r}
\]</span> hold for all <span class="math inline">\(n &gt; N\)</span>.</p></li>
<li><p>The policy <span class="math inline">\(\pi\)</span> is defined by <span class="math display">\[
\pi(s) = \underset{a}{\operatorname{argmax}}\{E(r \mid s, a) + \gamma \sum_{s'} p(s' \mid s, a) \cdot v^{n+1}(s')\}
\]</span>.</p></li>
<li><p><span class="math inline">\(\|v^{n+1} - v^*\| \leq \varepsilon / 2\)</span> when the condition <span class="math inline">\(\|v_\pi - v^*\| = \varepsilon\)</span> is met, where <span class="math inline">\(v_\pi\)</span> is <span class="math inline">\(\varepsilon\)</span>-optimal.</p></li>
</ol>
<p>The sequence <span class="math inline">\(\{v^n\}\)</span> is derived through the recursive equation <span class="math display">\[
v^{n+1} = LV^n
\]</span>, where <span class="math inline">\(L\)</span> represents the Bellman operator. The operator <span class="math inline">\(L\)</span> is defined as <span class="math display">\[
LV = \max_{\pi}\{r + \gamma P^{\pi}v\}
\]</span>, capturing the maximum expected sum of rewards under any policy.</p>
<p>In the theorem, <span class="math inline">\(\varepsilon\)</span> is used to set a threshold for the difference between consecutive values in the sequence <span class="math inline">\(\{v^n\}\)</span> during the iterative process of value iteration. Specifically, condition (b) states that for all finite <span class="math inline">\(N\)</span>, if <span class="math display">\[
\|v^{n+1} - v^n\| &lt; \varepsilon \times \frac{(1-\gamma)}{2\gamma}
\]</span> holds for all <span class="math inline">\(n &gt; N\)</span>, then the sequence is considered to have converged.</p>
<p>The theorem asserts the convergence of the sequence <span class="math inline">\(v^n\)</span> to the optimal value function <span class="math inline">\(v^*\)</span> under the conditions outlined in points (a) through (d). The proof involves analyzing the difference between consecutive values in the sequence and establishing convergence criteria.</p>
</section>
</section>
</section>
<section id="q-learning" class="level1">
<h1>Q-learning</h1>
<section id="introduction-2" class="level2">
<h2 class="anchored" data-anchor-id="introduction-2">Introduction</h2>
</section>
<section id="formulation-1" class="level2">
<h2 class="anchored" data-anchor-id="formulation-1">Formulation</h2>
</section>
</section>
<section id="sac" class="level1">
<h1>SAC</h1>
<section id="simple-overview" class="level2">
<h2 class="anchored" data-anchor-id="simple-overview">Simple overview</h2>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>